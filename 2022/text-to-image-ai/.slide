# Text to image AI
As a dev
22 Oct 2022
Tags: go, development
Summary: How does that work?

Tobias Theel
Princess and Lead Developer, Clarilab
tobias.theel@ClariLab.de
https://noobygames.de
@Nooby_Games

## Introduction

- Lots of funny apps.
- Did cost me several nights
- Wanted to understand how they do it

**Goal**: Provide a basic overview

## Example 1

.image assets/hacktoberfest.jpg _ 200
.caption Image generated by DREAM with keyword "hacktoberfest"

## Example 2

.image assets/1666031381062.jpg _ 300
.caption Image generated by DREAM with keywords "angel with flamesword"

## Example 3

.image assets/1666031373265.jpg _ 450

## Overview

> Yeah nice, but do you have some more examples?

> Are there any different methods?


> Do you know of more stuff?


> GIVE ME MORE IMAGES!!!!!111elf

## Google - DeepDream

.image assets/examples/google-deepdream.png 512 512

## Google - ImaGen

.image assets/examples/google-imagen.jpg 512 512

## OpenAI - Dall-E

.image assets/examples/dall-e.jpg 512 512

## OpenAI - Dall-E 2

.image assets/examples/dall-e-2.jpg 512 512

## Stability - Stable Diffusion

.image assets/examples/stable-diffusion.jpg 512 512

## Microsoft - NUWA-Infinity

.image assets/examples/NUWA-Infinity.png _ 1000

## VQGAN-CLIP: Open Domain Image Generation

.image assets/examples/vqgan+clip.png 512 512

## "Easy" vocabulary

.image assets/word-cloud.png _ 1000

## How it started

.image assets/man_dunking_basketball_infront_of_city.jpg 400 _
.caption image from: https://www.trainingsworld.com/wp-content/uploads/2013/01/dunking-dunk-lernen-uebungen.jpg

##

1. Key features
2. Key objects
3. Special attributes

## finding words

.image assets/man_dunking_basketball_infront_of_city_words.jpg 400 _
.caption image from: https://www.trainingsworld.com/wp-content/uploads/2013/01/dunking-dunk-lernen-uebungen.jpg edited by Theelinger

## building sentences

1. Man dunking basketball
2. White background
3. City with skyscrapers

## combine them

A **[man]** **[dunks]** a **[basketball]** infront of a **[city]** and **[white]** background

<br><br>

Objects: man, basketball, city

Features: white

Actions: dunks

## What if?!

.image assets/whatif-meme.jpg
.caption image generated by https://imgflip.com/memegenerator

## How to do the opposite?

Combine 2 neuronal networks.

1. Vector Quantized Generative Adversarial Network (VQGAN)
2. Contrastive Language-Image Pre-training (CLIP)

## CLIP

> "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. [...]"

Source: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)

## CLIP #2

1. CLIP knows how a Ball looks like
2. CLIP knows the "visual features" of a men
3. CLIP knows how to dunk a Basketball
4. CLIP is able to provide matching Text to every* image

*Every image in case, the model is trained on a huge basis.

## CLIP #3

.image assets/CLIP.png _ 1000

## CLIP Step 1

.image assets/CLIP_01.png _ 600

Take a huge pair of images & texts to train the model

## CLIP Step 2

.image assets/CLIP_02.png _ 600

Use unseen images to measure performance

## CLIP Step 2

Top predictions:

```
dog: 75.31%
turtle: 12.29%
sweet_pepper: 3.83%
lizard: 1.88%
crocodile: 1.75%
```

## VQGAN-CLIP

.image assets/VQGAN-CLIP.png _ 1000
.caption image taken from "VQGAN-CLIP Open Domain Image Generation" Paper https://arxiv.org/pdf/2204.08583.pdf

## Cool, cool, cool, can u show it in action?

YES! I can <3 (more or less)

## Iteration 01

.image assets/dog_playing_with_ball_01.png 500 500
.caption Iteration 01 - dog playing with ball

## Iteration 15

.image assets/dog_playing_with_ball_15.png 500 500
.caption Iteration 15 - dog playing with ball

## Iteration 75

.image assets/dog_playing_with_ball_75.png 500 500
.caption Iteration 75 - dog playing with ball

## Iteration 135

.image assets/dog_playing_with_ball_135.png 500 500
.caption Iteration 135 - dog playing with ball

## Iteration 240

.image assets/dog_playing_with_ball_240.png 500 500
.caption Iteration 240 - dog playing with ball

## Iteration 465

.image assets/dog_playing_with_ball_465.png 500 500
.caption Iteration 465 - dog playing with ball

## Iteration 720

.image assets/dog_playing_with_ball_720.png 500 500
.caption Iteration 720 - dog playing with ball


## Try it yourself?

Run on your on machine:

> [step by step guide](https://github.com/nerdyrodent/VQGAN-CLIP)

Run in google collab:

> [click through codebook](https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ#scrollTo=JX56bq4rEKIp)

Find Pre-trained models here:
> [taming-transformers](https://github.com/CompVis/taming-transformers)

Find a complete paper on the topic here:
> [taming-transformers-for-high-resolution-image-synthesis](https://arxiv.org/abs/2012.09841?amp=1)

Find the talk about the paper:
> [robin rombach taming tansformers for high resolution image synthesis](https://www.youtube.com/watch?v=fy153-yXSQk)

## Further further reading

[CLIP GitHub](https://github.com/openai/CLIP)

[CLIP-explained](https://www.youtube.com/watch?v=BcfAkQagEWU)